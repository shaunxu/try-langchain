from dotenv import load_dotenv
from langchain.chains.base import Chain
from langchain.chains import LLMChain, ConversationChain, RetrievalQA
from langchain.prompts.prompt import PromptTemplate
from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryBufferMemory
import os
from llms.chatglm import ChatGLM
import time
from typing import Union
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

load_dotenv()
llm_model_name_or_path = os.environ.get("LLM_MODEL_NAME_OR_PATH")
embedding_model_name_or_path = os.environ.get("EMBEDDING_MODEL_NAME_OR_PATH")
vectorstore_persist_directory = os.environ.get("VECTORSTORE_PERSIST_DIRECTORY")

def ask(chain: Chain, question: str) -> Union[str, float]:
    start = time.time()
    response = chain.predict(input=question)
    # response = chain.run(question)
    end = time.time()
    return [response, end - start]


def print_response(message: str, elasped: float) -> None:
    print(f"ğŸ¤–: {message}")
    print(f"ğŸ•—: {elasped}")

def main():
    llm = ChatGLM()
    llm.load_model(llm_model_name_or_path)

    embedding = HuggingFaceEmbeddings(model_name=embedding_model_name_or_path)
    db = Chroma(persist_directory=vectorstore_persist_directory, embedding_function=embedding)

    template = """ä½ çš„åå­—å«â€œå°Pâ€ï¼Œæ˜¯PingCodeæ™ºèƒ½åŠ©æ‰‹ï¼Œæ­£åœ¨å’Œäººç±»è¿›è¡Œå¤šè½®å¯¹è¯ã€‚
ä½ è¦ç®€å•æ‰¼è¦çš„å›ç­”äººç±»æå‡ºçš„é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯´æˆ‘ä¸çŸ¥é“ï¼Œä¸è¦æœæ’°ã€‚
æ³¨æ„ï¼Œå¦‚æœé—®é¢˜æˆ–ç­”æ¡ˆæ¶‰åŠæš´åŠ›ã€è‰²æƒ…ã€æ¯’å“ç­‰è¿æ³•çŠ¯ç½ªæƒ…å½¢ï¼Œè¯·æ‹’ç»å›ç­”ã€‚
åŒæ—¶ï¼Œè¯·æ§åˆ¶å›ç­”çš„é•¿åº¦ä¸è¦è¶…è¿‡200ä¸ªå­—ã€‚
ä»¥ä¸‹æ˜¯ä¹‹å‰é—®ç­”çš„ä¿¡æ¯ã€‚
{history}
ä»¥ä¸‹æ˜¯äººç±»æå‡ºçš„é—®é¢˜ã€‚
{input}
"""
    prompt = PromptTemplate(
        input_variables=["history", "input"],
        template=template
    )

    # chain = LLMChain(
    #     llm=llm,
    #     prompt=prompt,
    #     verbose=False
    # )

    chain = ConversationChain(
        llm=llm,
        prompt=prompt,
        # memory=ConversationBufferMemory(),
        memory=ConversationBufferWindowMemory(k=20),
        # memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=500),
        verbose=False
    )

    # chain = RetrievalQA(
    #     llm=llm,
    #     chain_type="stuff",
    #     retriever=db.as_retriever()
    # )
    # chain = RetrievalQA.from_chain_type(
    #     llm=llm, 
    #     chain_type="stuff",
    #     retriever=db.as_retriever()
    # )

    response, elasped = ask(chain, "ä½ å¥½")
    print_response(response, elasped)

    while True:
        question = input("ğŸ§‘â€ğŸ’¼: ")
        if question == "exit":
            print_response("Bye~", 0)
            break
        elif question == "":
            continue
        else:
            response, elasped = ask(chain, question)
            print_response(response, elasped)

            # start = time.time()
            # response = chain.predict(input=question)
            # end = time.time()
            # print_response(response, end - start)

if __name__ == "__main__":
    main()

# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
# model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).half().cuda()
# model = model.eval()

# prompt = "ä½ å¥½"
# print(f"Human: {prompt}")
# response, _ = model.chat(tokenizer, prompt, history=[], max_length=500, temperature=0.1)
# print(f"AI: {response}")
