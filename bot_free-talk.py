from dotenv import load_dotenv
import os
import datetime
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory
from llms.chatglm import ChatGLM
from llms.moss import Moss
from llms.aquilachat import AquilaChat
from llms.chinese_llama_alpaca import ChineseLlamaAlpaca
from langchain import HuggingFacePipeline
import torch

load_dotenv()
llm_model_name_or_path = os.environ.get("LLM_MODEL_NAME_OR_PATH")
embedding_model_name_or_path = os.environ.get("EMBEDDING_MODEL_NAME_OR_PATH")
vectorstore_persist_directory = os.environ.get("VECTORSTORE_PERSIST_DIRECTORY")

def print_with_timeframe(message: str) -> None:
    print(f"[{datetime.datetime.now()}] {message}")

def chat(chain: ConversationChain, question: str) -> None:
    response = chain.predict(input=question)
    print_with_timeframe(f"ğŸ¤–: {response}")

def main():
    prompt = PromptTemplate(
        template="""
ä½ çš„åå­—å«â€œå°Pâ€ï¼Œæ˜¯PingCodeæ™ºèƒ½åŠ©æ‰‹ï¼Œæ­£åœ¨å’Œäººç±»è¿›è¡Œå¤šè½®å¯¹è¯ã€‚
ä½ è¦ç®€å•æ‰¼è¦çš„å›ç­”äººç±»æå‡ºçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯´æˆ‘ä¸çŸ¥é“ï¼Œä¸è¦æœæ’°ã€‚
æ³¨æ„ï¼Œå¦‚æœé—®é¢˜æˆ–ç­”æ¡ˆæ¶‰åŠæš´åŠ›ã€è‰²æƒ…ã€æ¯’å“ç­‰è¿æ³•çŠ¯ç½ªæƒ…å½¢ï¼Œè¯·æ‹’ç»å›ç­”ã€‚
åŒæ—¶ï¼Œè¯·æ§åˆ¶å›ç­”çš„é•¿åº¦ä¸è¦è¶…è¿‡100ä¸ªå­—ã€‚
ä»¥ä¸‹æ˜¯ä¹‹å‰é—®ç­”çš„ä¿¡æ¯ï¼Œåœ¨<<<å’Œ>>>ä¹‹é—´ã€‚
<<<
{history}
>>>
ä»¥ä¸‹æ˜¯äººç±»æå‡ºçš„é—®é¢˜ï¼Œåœ¨è¿ç»­ä¸‰ä¸ªåå¼•å·ä¹‹é—´ã€‚
```
{input}
```
""",
        input_variables=["history", "input"],
    )

    print_with_timeframe(f"Initializing LLM from {llm_model_name_or_path}")
    # llm = ChatGLM()
    # llm = Moss()
    # llm = AquilaChat()
    # llm = ChineseLlamaAlpaca()
    # llm.load_model(llm_model_name_or_path)

    llm = HuggingFacePipeline.from_model_id(model_id=llm_model_name_or_path,
                                            task="text-generation",
                                            model_kwargs={
                                                "torch_dtype" : torch.float16,
                                                "low_cpu_mem_usage" : True,
                                                "temperature": 0.2,
                                                "max_length": 1000,
                                                "device_map": "auto",
                                                "repetition_penalty":1.1}
    )

    print_with_timeframe(f"LLM is ready. LLM={llm._llm_type}")

    print_with_timeframe(f"Initializing chain with prompt")
    chain = ConversationChain(
        llm=llm,
        prompt=prompt,
        memory=ConversationBufferWindowMemory(k=10),
        verbose=False
    )

    print_with_timeframe(f"Ready")
    chat(chain, "ä½ å¥½")
    while True:
        question = input(f"[{datetime.datetime.now()}] ğŸ§‘â€ğŸ’¼: ")
        if question == "exit":
            break
        elif question == "":
            continue
        else:
            chat(chain, question)

if __name__ == "__main__":
    main()
