from dotenv import load_dotenv
import os
import datetime
from llms.chatlgm import ChatGLM
from langchain.prompts.prompt import PromptTemplate
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory

load_dotenv()
llm_model_name_or_path = os.environ.get("LLM_MODEL_NAME_OR_PATH")
embedding_model_name_or_path = os.environ.get("EMBEDDING_MODEL_NAME_OR_PATH")
vectorstore_persist_directory = os.environ.get("VECTORSTORE_PERSIST_DIRECTORY")

def chat(chain: ConversationChain, question: str) -> None:
    response = chain.predict(input=question)
    print(f"[{datetime.datetime.now()}] ğŸ¤–: {response}")

def main():
    prompt = PromptTemplate(
        template="""
ä½ çš„åå­—å«â€œå°Pâ€ï¼Œæ˜¯PingCodeæ™ºèƒ½åŠ©æ‰‹ï¼Œæ­£åœ¨å’Œäººç±»è¿›è¡Œå¤šè½®å¯¹è¯ã€‚
ä½ è¦ç®€å•æ‰¼è¦çš„å›ç­”äººç±»æå‡ºçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯´æˆ‘ä¸çŸ¥é“ï¼Œä¸è¦æœæ’°ã€‚
æ³¨æ„ï¼Œå¦‚æœé—®é¢˜æˆ–ç­”æ¡ˆæ¶‰åŠæš´åŠ›ã€è‰²æƒ…ã€æ¯’å“ç­‰è¿æ³•çŠ¯ç½ªæƒ…å½¢ï¼Œè¯·æ‹’ç»å›ç­”ã€‚
åŒæ—¶ï¼Œè¯·æ§åˆ¶å›ç­”çš„é•¿åº¦ä¸è¦è¶…è¿‡100ä¸ªå­—ã€‚
ä»¥ä¸‹æ˜¯ä¹‹å‰é—®ç­”çš„ä¿¡æ¯ï¼Œåœ¨<<<å’Œ>>>ä¹‹é—´ã€‚
<<<
{history}
>>>
ä»¥ä¸‹æ˜¯äººç±»æå‡ºçš„é—®é¢˜ï¼Œåœ¨è¿ç»­ä¸‰ä¸ªåå¼•å·ä¹‹é—´ã€‚
```
{input}
```
""",
        input_variables=["history", "input"],
    )

    llm = ChatGLM()
    llm.load_model(llm_model_name_or_path)

    chain = ConversationChain(
        llm=llm,
        prompt=prompt,
        memory=ConversationBufferWindowMemory(k=10),
        verbose=False
    )

    chat(chain, "ä½ å¥½")
    while True:
        question = input(f"[{datetime.datetime.now()}] ğŸ§‘â€ğŸ’¼: ")
        if question == "exit":
            chat(chain, "å†è§")
            break
        elif question == "":
            continue
        else:
            chat(chain, question)

if __name__ == "__main__":
    main()
